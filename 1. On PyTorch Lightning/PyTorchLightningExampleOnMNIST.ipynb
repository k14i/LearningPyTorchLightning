{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch Lightning example on MNIST\n",
    "\n",
    "## References\n",
    "\n",
    "* [Step-by-step walk-through — PyTorch Lightning 1.5.0dev documentation](https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html)\n",
    "* [Introduction to Pytorch Lightning — PyTorch Lightning 1.5.0dev documentation](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/mnist-hello-world.html)\n",
    "* [Mastering-PyTorch/pytorch_lightning.ipynb at master · PacktPublishing/Mastering-PyTorch](https://github.com/PacktPublishing/Mastering-PyTorch/blob/master/Chapter14/pytorch_lightning.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "* [torch.nn.functional](https://pytorch.org/docs/stable/nn.functional.html)\n",
    "* [torchvision.datasets](https://pytorch.org/vision/stable/datasets.html)\n",
    "* [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)\n",
    "* [PyTorch Lightning Documentation — PyTorch Lightning 1.5.0dev documentation](https://pytorch-lightning.readthedocs.io/en/latest/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# For type hinting\n",
    "from typing import Union, Dict, List, Any\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "* [pytorch_lightning.LightningModule](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html)\n",
    "    - PyTorch のコードを 5つのセクションに整理する\n",
    "        1. Computations (init)\n",
    "        2. Train loop (training_step)\n",
    "        3. Validation loop (validation_step)\n",
    "        4. Test loop (test_step)\n",
    "        5. Optimizers (configure_optimizers)\n",
    "    - 特徴は以下の通り\n",
    "        1. PyTorch Lightning のコードは PyTorch のコードと同じである。\n",
    "        2. PyTorch のコードが抽象化されるわけではなく、整理される。\n",
    "        3. `LightningModule` に無いコードは `Trainer` によって自動化されている。\n",
    "        4. Lightning が処理をするため `.cuda()` や `.to()` といったコールは不要。\n",
    "        5. `DataLoader` において、デフォルトでは `DistributedSampler` が設定される。\n",
    "        6. `LightningModule` は `torch.nn.Module` の一つであり、それに機能を追加したもの。\n",
    "\n",
    "### `__init__`\n",
    "\n",
    "* [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    "* [torch.nn.MaxPool2d](https://pytorch.org/docs/1.9.0/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n",
    "* [torch.nn.Dropout2d](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d)\n",
    "* [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "\n",
    "### `forward`\n",
    "\n",
    "順伝播型ネットワークの定義および処理の実行\n",
    "\n",
    "* [forward](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html?highlight=forward#forward)\n",
    "    - [torch.nn.forward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward)\n",
    "* [torch.nn.functional.relu](https://pytorch.org/docs/1.9.0/generated/torch.nn.functional.relu.html)\n",
    "    - [torch.nn.ReLU](https://pytorch.org/docs/1.9.0/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
    "    - 正規化線形関数を適用\n",
    "* [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html)\n",
    "    - 次元を 1 + `start_dim` 次元に変更\n",
    "* [torch.nn.functional.log_softmax](https://pytorch.org/docs/1.9.0/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax)\n",
    "    - [torch.nn.LogSoftmax](https://pytorch.org/docs/1.9.0/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)\n",
    "    - ソフトマックス関数（活性化関数）を適用\n",
    "\n",
    "---\n",
    "\n",
    "### Training loop\n",
    "\n",
    "#### `training_step`\n",
    "\n",
    "学習を実行し、損失を返却する。 `train_dataloader` から学習用データを取得し、バッチごとに処理をする。処理には順伝播，勾配の最適化，逆伝播，パラメータの最適化が含まれる。\n",
    "GPUごとに処理をしたい場合は、これに加えて `training_step_end` をオーバーライドし、各GPUを利用した `training_step` の結果を結合する処理を記述する。\n",
    "\n",
    "* [training_step](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step)\n",
    "* [torch.nn.functional.cross_entropy](https://pytorch.org/docs/1.9.0/generated/torch.nn.functional.cross_entropy.html?highlight=cross_entropy#torch.nn.functional.cross_entropy)\n",
    "    - [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/1.9.0/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
    "* [LightningModule.log](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#log)\n",
    "\n",
    "#### `training_step_end`\n",
    "\n",
    "#### `training_epoch_end`\n",
    "\n",
    "---\n",
    "\n",
    "### Validation loop\n",
    "\n",
    "#### `validation_step`\n",
    "\n",
    "バリデーションを実行する。 `validation_epoch_end` への入力として集約したい値を返却する。ここでは汎化性能を確認するため、損失を返却する。\n",
    "\n",
    "* [validation_step](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#validation-step)\n",
    "\n",
    "#### `validation_step_end`\n",
    "\n",
    "* [validation_step_end](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#validation-step-end)\n",
    "\n",
    "#### `validation_epoch_end`\n",
    "\n",
    "バリデーションのエポック終了時にコールされる。すべての `validation_step` の出力を入力として受け取る。\n",
    "\n",
    "* [validation_epoch_end](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#validation-epoch-end)\n",
    "* [torch.stack](https://pytorch.org/docs/1.9.0/generated/torch.stack.html?highlight=stack#torch.stack)\n",
    "\n",
    "---\n",
    "\n",
    "### Test loop\n",
    "\n",
    "#### `test_step`\n",
    "\n",
    "テストを実行する。`test_epoch_end` への入力として集約したい値を返却する。ここでは汎化性能を確認するため、損失を返却する。\n",
    "テストループは `pytorch_lightning.Trainer.test(model)` が実行された場合のみ、実行される。\n",
    "\n",
    "* [test_step](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#test-step)\n",
    "\n",
    "#### `test_step_end`\n",
    "\n",
    "* [test_step_end](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#test-step-end)\n",
    "\n",
    "#### `test_epoch_end`\n",
    "\n",
    "テストのエポック終了時にコールされる。すべての `test_step` の出力を入力として受け取る。\n",
    "\n",
    "* [test_epoch_end](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#test-epoch-end)\n",
    "\n",
    "---\n",
    "\n",
    "### Prediction\n",
    "\n",
    "#### `predict_step`\n",
    "\n",
    "`pytorch_lightning.Trainer.predict(model)` が実行された場合のみ、実行される。\n",
    "\n",
    "* [predict_step](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#predict-step)\n",
    "\n",
    "---\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "#### `configure_optimizers`\n",
    "\n",
    "最適化のために使用するオプティマイザを選択する。\n",
    "\n",
    "* [configure_optimizers](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers)\n",
    "* [torch.optim.Adadelta](https://pytorch.org/docs/1.9.0/generated/torch.optim.Adadelta.html?highlight=adadelta#torch.optim.Adadelta)\n",
    "    - [[1212.5701] ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)\n",
    "\n",
    "---\n",
    "\n",
    "### DataLoaders\n",
    "\n",
    "#### `train_dataloader`\n",
    "\n",
    "* [train_dataloader](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#train-dataloader)\n",
    "* [torchvision.transforms.ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)\n",
    "* [torchvision.transforms.Normalize](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize)\n",
    "* [torchvision.transforms.Compose](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Compose)\n",
    "* [torchvision.datasets.MNIST](https://pytorch.org/vision/stable/datasets.html#mnist)\n",
    "* [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "\n",
    "#### `val_dataloader`\n",
    "\n",
    "* [val_dataloader](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#val-dataloader)\n",
    "\n",
    "#### `test_dataloader`\n",
    "\n",
    "* [test_dataloader](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#test-dataloader)\n",
    "\n",
    "---\n",
    "\n",
    "### Others\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class MNISTConvNet(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "            conv1_in_channels: int, conv1_out_channels: int, conv1_kernel_size: int, conv1_stride: int,\n",
    "            conv2_in_channels: int, conv2_out_channels: int, conv2_kernel_size: int, conv2_stride: int,\n",
    "            pool1_kernel_size: int, dropout1_p: float, dropout2_p: float,\n",
    "            fullconn1_in_features: int, fullconn1_out_features: int, fullconn2_in_features: int, fullconn2_out_features: int,\n",
    "            adadelta_lr: float, adadelta_rho: float, adadelta_eps: float, adadelta_weight_decay: float,\n",
    "            dataset_root: str, dataset_download: bool,\n",
    "            dataloader_mean: tuple, dataloader_std: tuple, dataloader_batch_size: int, dataloader_num_workers: int\n",
    "            ) -> None:\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=conv1_in_channels, out_channels=conv1_out_channels, kernel_size=conv1_kernel_size, stride=conv1_stride)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=conv2_in_channels, out_channels=conv2_out_channels, kernel_size=conv2_kernel_size, stride=conv2_stride)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=pool1_kernel_size)\n",
    "        self.dropout1 = torch.nn.Dropout2d(p=dropout1_p, inplace=False)\n",
    "        self.dropout2 = torch.nn.Dropout2d(p=dropout2_p, inplace=False)\n",
    "        self.fullconn1 = torch.nn.Linear(in_features=fullconn1_in_features, out_features=fullconn1_out_features)\n",
    "        self.fullconn2 = torch.nn.Linear(in_features=fullconn2_in_features, out_features=fullconn2_out_features)\n",
    "\n",
    "        self.adadelta_params = {\n",
    "            'lr': adadelta_lr,\n",
    "            'rho': adadelta_rho,\n",
    "            'eps': adadelta_eps,\n",
    "            'weight_decay': adadelta_weight_decay,\n",
    "        }\n",
    "\n",
    "        self.dataset_params = {\n",
    "            'root': dataset_root,\n",
    "            'download': dataset_download,\n",
    "        }\n",
    "\n",
    "        self.dataloader_params = {\n",
    "            'mean': dataloader_mean,\n",
    "            'std': dataloader_std,\n",
    "            'batch_size': dataloader_batch_size,\n",
    "            'num_workers': dataloader_num_workers,\n",
    "        }\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(input=x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(input=x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(input=x, start_dim=1)\n",
    "        x = self.fullconn1(x)\n",
    "        x = F.relu(input=x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fullconn2(x)\n",
    "        return F.log_softmax(input=x, dim=1)\n",
    "    \n",
    "    def _common_step(self, batch: Any, log_name: str,\n",
    "            log_on_step: Any = None, log_on_epoch: Any = None, log_prog_bar: bool = False\n",
    "            ) -> Union[Tensor, Dict[str, Any], None]:\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(input=y_pred, target=y)\n",
    "\n",
    "        self.log(name=log_name, value=loss, prog_bar=log_prog_bar, on_step=log_on_step, on_epoch=log_on_epoch)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "\n",
    "    # def on_train_epoch_start(self) -> None:\n",
    "    #     return super().on_train_epoch_start()\n",
    "\n",
    "    # def on_train_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "    #     return super().on_train_batch_start(batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Union[Tensor, Dict[str, Any]]:\n",
    "        return self._common_step(batch=batch, log_name=\"train_loss\", log_prog_bar=True, log_on_epoch=True)\n",
    "    \n",
    "    # def on_train_batch_end(self, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "    #     return super().on_train_batch_end(outputs, batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    # def training_step_end(self, *args, **kwargs) -> Any:\n",
    "    #     return super().training_step_end(*args, **kwargs)\n",
    "    \n",
    "    # def training_epoch_end(self, outputs: Any) -> None:\n",
    "    #     return super().training_epoch_end(outputs)\n",
    "\n",
    "    # def on_train_epoch_end(self) -> None:\n",
    "    #     return super().on_train_epoch_end()\n",
    "\n",
    "\n",
    "    # Validation loop\n",
    "\n",
    "    # def on_validation_epoch_start(self) -> None:\n",
    "    #     return super().on_validation_epoch_start()\n",
    "\n",
    "    # def on_validation_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "    #     return super().on_validation_batch_start(batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> Union[Tensor, Dict[str, Any], None]:\n",
    "        return self._common_step(batch=batch, log_name=\"val_loss\", log_prog_bar=True, log_on_step=True, log_on_epoch=True)\n",
    "    \n",
    "    # def on_validation_batch_end(self, outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "    #     return super().on_validation_batch_end(outputs, batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    # def validation_step_end(self, *args, **kwargs) -> Optional[Any]:\n",
    "    #     return super().validation_step_end(*args, **kwargs)\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Union[Tensor, Dict[str, Any]]] ) -> None:\n",
    "        return torch.stack(tensors=outputs).mean() # NOTE: Average loss\n",
    "    \n",
    "    # def on_validation_epoch_end(self) -> None:\n",
    "    #     return super().on_validation_epoch_end()\n",
    "\n",
    "\n",
    "    # Test loop\n",
    "\n",
    "    # def on_test_epoch_start(self) -> None:\n",
    "    #     return super().on_test_epoch_start()\n",
    "\n",
    "    # def on_test_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "    #     return super().on_test_batch_start(batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int) -> Union[Tensor, Dict[str, Any], None]:\n",
    "        return self._common_step(batch=batch, log_name=\"test_loss\", log_prog_bar=True, log_on_step=True, log_on_epoch=True)\n",
    "    \n",
    "    # def on_test_batch_end(self, outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "    #     return super().on_test_batch_end(outputs, batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    # def test_step_end(self, *args, **kwargs) -> Optional[Any]:\n",
    "    #     return super().test_step_end(*args, **kwargs)\n",
    "    \n",
    "    def test_epoch_end(self, outputs: List[Union[Tensor, Dict[str, Any]]] ) -> None:\n",
    "        return torch.stack(tensors=outputs).mean() # NOTE: Average loss\n",
    "    \n",
    "    # def on_test_epoch_end(self) -> None:\n",
    "    #     return super().on_test_epoch_end()\n",
    "    \n",
    "\n",
    "    # Prediction\n",
    "\n",
    "    # def on_predict_epoch_start(self) -> None:\n",
    "    #     return super().on_predict_epoch_start()\n",
    "\n",
    "    # def on_predict_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "    #     return super().on_predict_batch_start(batch, batch_idx, dataloader_idx)\n",
    "\n",
    "    # def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> Any:\n",
    "    #     return super().predict_step(batch, batch_idx, dataloader_idx=dataloader_idx)\n",
    "    \n",
    "    # def on_predict_batch_end(self, outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int) -> None:\n",
    "    #     return super().on_predict_batch_end(outputs, batch, batch_idx, dataloader_idx)\n",
    "    \n",
    "    # def on_predict_epoch_end(self, results: List[Any]) -> None:\n",
    "    #     return super().on_predict_epoch_end(results)\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        return torch.optim.Adadelta(params=self.parameters(),\n",
    "            lr=self.adadelta_params['lr'],\n",
    "            rho=self.adadelta_params['rho'],\n",
    "            eps=self.adadelta_params['eps'],\n",
    "            weight_decay=self.adadelta_params['weight_decay'])\n",
    "    \n",
    "    # def on_before_optimizer_step(self, optimizer: Any, optimizer_idx: int) -> None:\n",
    "    #     return super().on_before_optimizer_step(optimizer, optimizer_idx)\n",
    "\n",
    "    # def optimizer_step(self, epoch: int, batch_idx: int, optimizer: Any, optimizer_idx: int, optimizer_closure: Optional[Any], on_tpu: bool, using_native_amp: bool, using_lbfgs: bool) -> None:\n",
    "    #     return super().optimizer_step(epoch=epoch, batch_idx=batch_idx, optimizer=optimizer, optimizer_idx=optimizer_idx, optimizer_closure=optimizer_closure, on_tpu=on_tpu, using_native_amp=using_native_amp, using_lbfgs=using_lbfgs)\n",
    "\n",
    "\n",
    "    # Dataloaders\n",
    "\n",
    "    def _get_dataloader(self, train: bool) -> Any:\n",
    "        transform_objects = [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=self.dataloader_params['mean'], std=self.dataloader_params['std'])\n",
    "        ]\n",
    "        transform = torchvision.transforms.Compose(transforms=transform_objects)\n",
    "        dataset = torchvision.datasets.MNIST(root=self.dataset_params['root'],\n",
    "            train=train,\n",
    "            download=self.dataset_params['download'],\n",
    "            transform=transform)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "            batch_size=self.dataloader_params['batch_size'],\n",
    "            num_workers=self.dataloader_params['num_workers'])\n",
    "        return dataloader\n",
    "\n",
    "    # def on_train_dataloader(self) -> None:\n",
    "    #     return super().on_train_dataloader()\n",
    "\n",
    "    def train_dataloader(self) -> Any:\n",
    "        return self._get_dataloader(train=True)\n",
    "    \n",
    "    # def on_val_dataloader(self) -> None:\n",
    "    #     return super().on_val_dataloader()\n",
    "\n",
    "    def val_dataloader(self) -> Any:\n",
    "        return self._get_dataloader(train=True)\n",
    "    \n",
    "    # def on_test_dataloader(self) -> None:\n",
    "    #     return super().on_test_dataloader()\n",
    "\n",
    "    def test_dataloader(self) -> Any:\n",
    "        return self._get_dataloader(train=False)\n",
    "    \n",
    "    # def on_predict_dataloader(self) -> None:\n",
    "    #     return super().on_predict_dataloader()\n",
    "\n",
    "    # def predict_dataloader(self) -> Any:\n",
    "    #     return super().predict_dataloader()\n",
    "\n",
    "\n",
    "    # Others\n",
    "\n",
    "    # def setup(self, stage: Optional[str]) -> None:\n",
    "    #     return super().setup(stage=stage)\n",
    "\n",
    "    # def teardown(self, stage: Optional[str]) -> None:\n",
    "    #     return super().teardown(stage=stage)\n",
    "    \n",
    "    # def prepare_data(self) -> None:\n",
    "    #     return super().prepare_data()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_argparser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Lightning MNIST Example')\n",
    "    parser.add_argument('--conv1-in-channels', type=int, default=1)\n",
    "    parser.add_argument('--conv1-out-channels', type=int, default=32)\n",
    "    parser.add_argument('--conv1-kernel-size', type=int, default=3)\n",
    "    parser.add_argument('--conv1-stride', type=int, default=1)\n",
    "    parser.add_argument('--conv2-in-channels', type=int, default=32)\n",
    "    parser.add_argument('--conv2-out-channels', type=int, default=64)\n",
    "    parser.add_argument('--conv2-kernel-size', type=int, default=3)\n",
    "    parser.add_argument('--conv2-stride', type=int, default=1)\n",
    "    parser.add_argument('--pool1-kernel-size', type=int, default=2)\n",
    "    parser.add_argument('--dropout1-p', type=float, default=0.25)\n",
    "    parser.add_argument('--dropout2-p', type=float, default=0.5)\n",
    "    parser.add_argument('--fullconn1-in-features', type=int, default=12*12*64)\n",
    "    parser.add_argument('--fullconn1-out-features', type=int, default=128)\n",
    "    parser.add_argument('--fullconn2-in-features', type=int, default=128)\n",
    "    parser.add_argument('--fullconn2-out-features', type=int, default=10)\n",
    "    parser.add_argument('--adadelta-lr', type=float, default=1.0)\n",
    "    parser.add_argument('--adadelta-rho', type=float, default=0.9)\n",
    "    parser.add_argument('--adadelta-eps', type=float, default=1e-06)\n",
    "    parser.add_argument('--adadelta-weight-decay', type=float, default=0)\n",
    "    parser.add_argument('--dataset-root', type=str, default=os.getcwd())\n",
    "    parser.add_argument('--dataset-download', action='store_true', default=True)\n",
    "    parser.add_argument('--dataloader-mean', type=tuple, default=(0.1302,))\n",
    "    parser.add_argument('--dataloader-std', type=tuple, default=(0.3069,))\n",
    "    parser.add_argument('--dataloader-batch-size', type=int, default=32)\n",
    "    parser.add_argument('--dataloader-num-workers', type=int, default=4)\n",
    "    parser.add_argument('--progress-bar-refresh-rate', type=int, default=20)\n",
    "    parser.add_argument('--max-epochs', type=int, default=1)\n",
    "    return parser"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "* [pytorch_lightning.Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html?highlight=trainer)\n",
    "    - [fit](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fit)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def main(args=None) -> None:\n",
    "    if not args:\n",
    "        args = get_argparser().parse_args()\n",
    "    \n",
    "    model = MNISTConvNet(conv1_in_channels=args.conv1_in_channels, conv1_out_channels=args.conv1_out_channels, conv1_kernel_size=args.conv1_kernel_size, conv1_stride=args.conv1_stride,\n",
    "        conv2_in_channels=args.conv2_in_channels, conv2_out_channels=args.conv2_out_channels, conv2_kernel_size=args.conv2_kernel_size, conv2_stride=args.conv2_stride,\n",
    "        pool1_kernel_size=args.pool1_kernel_size, dropout1_p=args.dropout1_p, dropout2_p=args.dropout2_p,\n",
    "        fullconn1_in_features=args.fullconn1_in_features, fullconn1_out_features=args.fullconn1_out_features, fullconn2_in_features=args.fullconn2_in_features, fullconn2_out_features=args.fullconn2_out_features,\n",
    "        adadelta_lr=args.adadelta_lr, adadelta_rho=args.adadelta_rho, adadelta_eps=args.adadelta_eps, adadelta_weight_decay=args.adadelta_weight_decay,\n",
    "        dataset_root=args.dataset_root, dataset_download=args.dataset_download,\n",
    "        dataloader_mean=args.dataloader_mean, dataloader_std=args.dataloader_std, dataloader_batch_size=args.dataloader_batch_size, dataloader_num_workers=args.dataloader_num_workers)\n",
    "    \n",
    "    trainer = pl.Trainer(progress_bar_refresh_rate=args.progress_bar_refresh_rate, max_epochs=args.max_epochs)\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "    # trainer.predict(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    argparser = get_argparser()\n",
    "    args = argparser.parse_args([\n",
    "        \"--max-epochs\", str(1),\n",
    "        \"--adadelta-lr\", str(0.5),\n",
    "    ])\n",
    "    main(args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | conv1     | Conv2d    | 320   \n",
      "1 | conv2     | Conv2d    | 18.5 K\n",
      "2 | pool1     | MaxPool2d | 0     \n",
      "3 | dropout1  | Dropout2d | 0     \n",
      "4 | dropout2  | Dropout2d | 0     \n",
      "5 | fullconn1 | Linear    | 1.2 M \n",
      "6 | fullconn2 | Linear    | 1.3 K \n",
      "----------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.800     Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: 100%|██████████| 3750/3750 [01:41<00:00, 37.06it/s, loss=nan, v_num=54, train_loss_step=nan.0, val_loss_step=nan.0, val_loss_epoch=nan.0]\n",
      "Testing: 100%|██████████| 313/313 [00:04<00:00, 80.02it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': nan, 'test_loss_epoch': nan}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 313/313 [00:04<00:00, 75.81it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('2021-08-15': pyenv)"
  },
  "interpreter": {
   "hash": "fb3d78d3dd3d239ce1d9331482d71454fb874f5e0ee61df069f7324525ebef67"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
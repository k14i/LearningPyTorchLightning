{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chainer exmaple on MNIST\n",
    "\n",
    "## References\n",
    "\n",
    "* [chainer/examples/mnist at master Â· chainer/chainer](https://github.com/chainer/chainer/tree/master/examples/mnist)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import argparse\n",
    "import chainer\n",
    "import chainer.functions as F\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MNISTConvNet(chainer.Chain):\n",
    "    def __init__(self, n_units, n_out,\n",
    "            conv1_in_channels: int, conv1_out_channels: int, conv1_kernel_size: int, conv1_stride: int,\n",
    "            conv2_in_channels: int, conv2_out_channels: int, conv2_kernel_size: int, conv2_stride: int,\n",
    "            pool1_kernel_size: int, dropout1_p: float, dropout2_p: float,\n",
    "            fullconn1_in_features: int, fullconn1_out_features: int, fullconn2_in_features: int, fullconn2_out_features: int\n",
    "            ) -> None:\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = chainer.links.Convolution2D(in_channels=conv1_in_channels, out_channels=conv1_out_channels, ksize=conv1_kernel_size, stride=conv1_stride)\n",
    "            self.conv2 = chainer.links.Convolution2D(in_channels=conv2_in_channels, out_channels=conv2_out_channels, ksize=conv2_kernel_size, stride=conv2_stride)\n",
    "            self.fullconn1 = chainer.links.Linear(in_size=fullconn1_in_features, out_size=fullconn1_out_features)\n",
    "            self.fullconn2 = chainer.links.Linear(in_size=fullconn2_in_features, out_size=fullconn2_out_features)\n",
    "\n",
    "            self.pool1_kernel_size = pool1_kernel_size\n",
    "            self.dropout1_p = dropout1_p\n",
    "            self.dropout2_p = dropout2_p\n",
    "        \n",
    "    def __call__(self, x):  # NOTE: ...Or, def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pooling_2d(x, ksize=self.pool1_kernel_size)\n",
    "        x = F.dropout(x, ratio=self.dropout1_p)\n",
    "        x = F.flatten(x)\n",
    "        x = self.fullconn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, ratio=self.dropout2_p)\n",
    "        x = self.fullconn2(x)\n",
    "        return F.log_softmax(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def set_printing_parameters(trainer):\n",
    "    trainer.extend(chainer.training.extensions.LogReport(), call_before_training=True)\n",
    "    trainer.extend(chainer.training.extensions.PlotReport(y_keys=['main/loss', 'validation/main/loss'], x_key='epoch', filename='loss.png'), call_before_training=True)\n",
    "    trainer.extend(chainer.training.extensions.PlotReport(y_keys=['main/accuracy', 'validation/main/accuracy'], x_key='epoch', filename='accuracy.png'), call_before_training=True)\n",
    "    trainer.extend(chainer.training.extensions.PrintReport(entries=['epoch', 'main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy', 'elapsed_time']), call_before_training=True)\n",
    "    trainer.extend(chainer.training.extensions.ProgressBar())\n",
    "    return trainer\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_trainer(model, dataset, batchsize, device, epoch, out):\n",
    "    optimizer = chainer.optimizers.AdaDelta()\n",
    "    optimizer.setup(link=model)\n",
    "    \n",
    "    train_iter = chainer.iterators.SerialIterator(dataset=dataset, batch_size=batchsize, repeat=True, shuffle=None, order_sampler=None)\n",
    "\n",
    "    updater = chainer.training.updaters.StandardUpdater(iterator=train_iter, optimizer=optimizer, device=device, loss_func=None, loss_scale=None, auto_new_epoch=True)\n",
    "    trainer = chainer.training.Trainer(updater=updater, stop_trigger=(epoch, 'epoch'), out=out)\n",
    "    return trainer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare_trainer(trainer, device, model, dataset, batchsize, epoch, frequency, resume=None, autoload=True):\n",
    "    test_iter = chainer.iterators.SerialIterator(dataset=dataset, batch_size=batchsize, repeat=False, shuffle=False, order_sampler=None)\n",
    "    trainer.extend(extension=chainer.training.extensions.Evaluator(iterator=test_iter, target=model, device=device, eval_hook=None, eval_func=None),\n",
    "        call_before_training=True)\n",
    "    \n",
    "    frequency = epoch if frequency == -1 else max(1, frequency)\n",
    "    trainer.extend(chainer.training.extensions.snapshot(n_retains=1, autoload=autoload), name=None, trigger=(frequency, 'epoch'), priority=None, call_before_training=False)\n",
    "\n",
    "    trainer = set_printing_parameters(trainer=trainer)\n",
    "\n",
    "    if resume is not None:\n",
    "        chainer.serializers.load_npz(file=resume, obj=trainer)\n",
    "    \n",
    "    return trainer\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(predictor, device, unit, batchsize, epoch, out, frequency, resume=None, autoload=True) -> None:\n",
    "    device = chainer.get_device(device_spec=device)\n",
    "\n",
    "    print('Device: {}'.format(device))\n",
    "    print('# unit: {}'.format(unit))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('')\n",
    "\n",
    "    model = chainer.links.Classifier(predictor=predictor)\n",
    "    model.to_device(device=device)\n",
    "    device.use()\n",
    "\n",
    "    train_dataset, test_dataset = chainer.datasets.get_mnist()\n",
    "\n",
    "    trainer = get_trainer(model=model, dataset=train_dataset, batchsize=batchsize, device=device, epoch=epoch, out=out)\n",
    "    trainer = prepare_trainer(trainer=trainer, device=device, model=model, dataset=test_dataset, batchsize=batchsize, epoch=epoch, frequency=frequency, resume=resume, autoload=autoload)\n",
    "\n",
    "    trainer.run()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def predict(predictor, device, unit, snapshot) -> None:\n",
    "    device = chainer.get_device(device_spec=device)\n",
    "\n",
    "    print('Device: {}'.format(device))\n",
    "    print('# unit: {}'.format(unit))\n",
    "    print('')\n",
    "\n",
    "    device.use()\n",
    "\n",
    "    model = predictor\n",
    "\n",
    "    try:\n",
    "        chainer.serializers.load_npz(file=snapshot, obj=model, path='updater/model:main/predictor/', strict=True, ignore_names=None)\n",
    "    except Exception:\n",
    "        chainer.serializers.load_npz(file=snapshot, obj=model, path='predictor/', strict=True, ignore_names=None)\n",
    "    \n",
    "    model.to_device(device=device)\n",
    "\n",
    "    _, test_dataset = chainer.datasets.get_mnist()\n",
    "\n",
    "    x, answer = test_dataset[0]\n",
    "    x = device.send(arrays=x)\n",
    "    with chainer.using_config(name='train', value=False):\n",
    "        prediction = model(x[None, ...])[0].array.argmax()\n",
    "    \n",
    "    print('Prediction:', prediction)\n",
    "    print('Answer:', answer)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_argparser():\n",
    "    parser = argparse.ArgumentParser(description='Chainer example: MNIST')\n",
    "\n",
    "    # For the model\n",
    "    parser.add_argument('--conv1-in-channels', type=int, default=1)\n",
    "    parser.add_argument('--conv1-out-channels', type=int, default=32)\n",
    "    parser.add_argument('--conv1-kernel-size', type=int, default=3)\n",
    "    parser.add_argument('--conv1-stride', type=int, default=1)\n",
    "    parser.add_argument('--conv2-in-channels', type=int, default=32)\n",
    "    parser.add_argument('--conv2-out-channels', type=int, default=64)\n",
    "    parser.add_argument('--conv2-kernel-size', type=int, default=3)\n",
    "    parser.add_argument('--conv2-stride', type=int, default=1)\n",
    "    parser.add_argument('--pool1-kernel-size', type=int, default=2)\n",
    "    parser.add_argument('--dropout1-p', type=float, default=0.25)\n",
    "    parser.add_argument('--dropout2-p', type=float, default=0.5)\n",
    "    parser.add_argument('--fullconn1-in-features', type=int, default=12*12*64)\n",
    "    parser.add_argument('--fullconn1-out-features', type=int, default=128)\n",
    "    parser.add_argument('--fullconn2-in-features', type=int, default=128)\n",
    "    parser.add_argument('--fullconn2-out-features', type=int, default=10)\n",
    "\n",
    "    # Both for training and inference\n",
    "    parser.add_argument('--device', '-d', type=str, default='-1',\n",
    "                        help='Device specifier. Either ChainerX device '\n",
    "                        'specifier or an integer. If non-negative integer, '\n",
    "                        'CuPy arrays with specified device id are used. If '\n",
    "                        'negative integer, NumPy arrays are used')\n",
    "    parser.add_argument('--unit', '-u', type=int, default=128,\n",
    "                        help='Number of units')\n",
    "    group = parser.add_argument_group('deprecated arguments')\n",
    "    group.add_argument('--gpu', '-g', dest='device',\n",
    "                       type=int, nargs='?', const=0,\n",
    "                       help='GPU ID (negative value indicates CPU)')\n",
    "    \n",
    "    # For training\n",
    "    parser.add_argument('--batchsize', '-b', type=int, default=32,\n",
    "                        help='Number of images in each mini-batch')\n",
    "    parser.add_argument('--epoch', '-e', type=int, default=1,\n",
    "                        help='Number of sweeps over the dataset to train')\n",
    "    parser.add_argument('--frequency', '-f', type=int, default=-1,\n",
    "                        help='Frequency of taking a snapshot')\n",
    "    parser.add_argument('--out', '-o', default='result',\n",
    "                        help='Directory to output the result')\n",
    "    parser.add_argument('--resume', '-r', type=str,\n",
    "                        help='Resume the training from snapshot')\n",
    "    parser.add_argument('--autoload', action='store_true',\n",
    "                        help='Automatically load trainer snapshots in case'\n",
    "                        ' of preemption or other temporary system failure')\n",
    "    \n",
    "    # For inference\n",
    "    parser.add_argument('--snapshot', '-s',\n",
    "                        default='result/snapshot_iter_12000',\n",
    "                        help='The path to a saved snapshot (NPZ)')\n",
    "\n",
    "    return parser, group\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def main(args=None) -> None:\n",
    "    if not args:\n",
    "        argparser, _ = get_argparser()\n",
    "        args = argparser.parse_args()\n",
    "    \n",
    "    predictor = MNISTConvNet(n_units=args.unit, n_out=10,\n",
    "        conv1_in_channels=args.conv1_in_channels, conv1_out_channels=args.conv1_out_channels, conv1_kernel_size=args.conv1_kernel_size, conv1_stride=args.conv1_stride,\n",
    "        conv2_in_channels=args.conv2_in_channels, conv2_out_channels=args.conv2_out_channels, conv2_kernel_size=args.conv2_kernel_size, conv2_stride=args.conv2_stride,\n",
    "        pool1_kernel_size=args.pool1_kernel_size, dropout1_p=args.dropout1_p, dropout2_p=args.dropout2_p,\n",
    "        fullconn1_in_features=args.fullconn1_in_features, fullconn1_out_features=args.fullconn1_out_features, fullconn2_in_features=args.fullconn2_in_features, fullconn2_out_features=args.fullconn2_out_features\n",
    "        )\n",
    "\n",
    "    train(predictor=predictor, device=args.device, unit=args.unit, batchsize=args.batchsize, epoch=args.epoch, out=args.out, frequency=args.frequency)\n",
    "    predict(predictor=predictor, device=args.device, unit=args.unit, snapshot=args.snapshot)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "argparser, _ = get_argparser()\n",
    "args = argparser.parse_args(\n",
    "    [\n",
    "        \"--device\", str(-1),\n",
    "        \"--unit\", str(128),\n",
    "        # \"--gpu\", str(0),\n",
    "        \"--batchsize\", str(32),\n",
    "        \"--epoch\", str(1),\n",
    "        \"--frequency\", str(1),\n",
    "        \"--out\", \"result\",\n",
    "        # \"--resume\", \"\",\n",
    "        \"--autoload\",\n",
    "        \"--snapshot\", \"result/snapshot_iter_3750\",\n",
    "    ]\n",
    ")\n",
    "main(args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('2021-08-09': conda)"
  },
  "interpreter": {
   "hash": "e70296657866dcab8c1fff77bde7d1eee3f4d180a603dc3f836290cb0427c466"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
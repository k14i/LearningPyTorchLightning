{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch example on MNIST\n",
    "\n",
    "## References\n",
    "\n",
    "* [examples/mnist at master Â· pytorch/examples](https://github.com/pytorch/examples/blob/master/mnist/main.py)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# For type hinting\n",
    "from typing import Union, Dict, List, Any\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "* [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "\n",
    "### `__init__`\n",
    "\n",
    "* [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    "* [torch.nn.MaxPool2d](https://pytorch.org/docs/1.9.0/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n",
    "* [torch.nn.Dropout2d](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d)\n",
    "* [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "\n",
    "### `forward`\n",
    "\n",
    "* [forward](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html?highlight=forward#forward)\n",
    "    - [torch.nn.forward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward)\n",
    "* [torch.nn.functional.relu](https://pytorch.org/docs/1.9.0/generated/torch.nn.functional.relu.html)\n",
    "    - [torch.nn.ReLU](https://pytorch.org/docs/1.9.0/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
    "* [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html)\n",
    "* [torch.nn.functional.log_softmax](https://pytorch.org/docs/1.9.0/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax)\n",
    "    - [torch.nn.LogSoftmax](https://pytorch.org/docs/1.9.0/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MNISTConvNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "            conv1_in_channels: int, conv1_out_channels: int, conv1_kernel_size: int, conv1_stride: int,\n",
    "            conv2_in_channels: int, conv2_out_channels: int, conv2_kernel_size: int, conv2_stride: int,\n",
    "            pool1_kernel_size: int, dropout1_p: float, dropout2_p: float,\n",
    "            fullconn1_in_features: int, fullconn1_out_features: int, fullconn2_in_features: int, fullconn2_out_features: int,\n",
    "            adadelta_lr: float, adadelta_rho: float, adadelta_eps: float, adadelta_weight_decay: float,\n",
    "            dataset_root: str, dataset_download: bool,\n",
    "            dataloader_mean: tuple, dataloader_std: tuple, dataloader_batch_size: int, dataloader_num_workers: int\n",
    "            ) -> None:\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=conv1_in_channels, out_channels=conv1_out_channels, kernel_size=conv1_kernel_size, stride=conv1_stride)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=conv2_in_channels, out_channels=conv2_out_channels, kernel_size=conv2_kernel_size, stride=conv2_stride)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=pool1_kernel_size)\n",
    "        self.dropout1 = torch.nn.Dropout2d(p=dropout1_p, inplace=False)\n",
    "        self.dropout2 = torch.nn.Dropout2d(p=dropout2_p, inplace=False)\n",
    "        self.fullconn1 = torch.nn.Linear(in_features=fullconn1_in_features, out_features=fullconn1_out_features)\n",
    "        self.fullconn2 = torch.nn.Linear(in_features=fullconn2_in_features, out_features=fullconn2_out_features)\n",
    "\n",
    "        self.adadelta_params = {\n",
    "            'lr': adadelta_lr,\n",
    "            'rho': adadelta_rho,\n",
    "            'eps': adadelta_eps,\n",
    "            'weight_decay': adadelta_weight_decay,\n",
    "        }\n",
    "\n",
    "        self.dataset_params = {\n",
    "            'root': dataset_root,\n",
    "            'download': dataset_download,\n",
    "        }\n",
    "\n",
    "        self.dataloader_params = {\n",
    "            'mean': dataloader_mean,\n",
    "            'std': dataloader_std,\n",
    "            'batch_size': dataloader_batch_size,\n",
    "            'num_workers': dataloader_num_workers,\n",
    "        }\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(input=x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(input=x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(input=x, start_dim=1)\n",
    "        x = self.fullconn1(x)\n",
    "        x = F.relu(input=x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fullconn2(x)\n",
    "        return F.log_softmax(input=x, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch) -> None:\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test(model, device, test_loader) -> None:\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_argparser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--conv1-in-channels', type=int, default=1)\n",
    "    parser.add_argument('--conv1-out-channels', type=int, default=32)\n",
    "    parser.add_argument('--conv1-kernel-size', type=int, default=3)\n",
    "    parser.add_argument('--conv1-stride', type=int, default=1)\n",
    "    parser.add_argument('--conv2-in-channels', type=int, default=32)\n",
    "    parser.add_argument('--conv2-out-channels', type=int, default=64)\n",
    "    parser.add_argument('--conv2-kernel-size', type=int, default=3)\n",
    "    parser.add_argument('--conv2-stride', type=int, default=1)\n",
    "    parser.add_argument('--pool1-kernel-size', type=int, default=2)\n",
    "    parser.add_argument('--dropout1-p', type=float, default=0.25)\n",
    "    parser.add_argument('--dropout2-p', type=float, default=0.5)\n",
    "    parser.add_argument('--fullconn1-in-features', type=int, default=12*12*64)\n",
    "    parser.add_argument('--fullconn1-out-features', type=int, default=128)\n",
    "    parser.add_argument('--fullconn2-in-features', type=int, default=128)\n",
    "    parser.add_argument('--fullconn2-out-features', type=int, default=10)\n",
    "    parser.add_argument('--adadelta-lr', type=float, default=1.0)\n",
    "    parser.add_argument('--adadelta-rho', type=float, default=0.9)\n",
    "    parser.add_argument('--adadelta-eps', type=float, default=1e-06)\n",
    "    parser.add_argument('--adadelta-weight-decay', type=float, default=0)\n",
    "    parser.add_argument('--dataset-root', type=str, default=os.getcwd())\n",
    "    parser.add_argument('--dataset-download', action='store_true', default=True)\n",
    "    parser.add_argument('--dataloader-mean', type=tuple, default=(0.1302,))\n",
    "    parser.add_argument('--dataloader-std', type=tuple, default=(0.3069,))\n",
    "    parser.add_argument('--dataloader-batch-size', type=int, default=32)\n",
    "    parser.add_argument('--dataloader-num-workers', type=int, default=4)\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    return parser"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def main(args=None) -> None:\n",
    "    if not args:\n",
    "        args = get_argparser().parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {\n",
    "            'num_worksers': 1,\n",
    "            'pin_memory': True,\n",
    "            'shuffle': True,\n",
    "        }\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    \n",
    "    tensor = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.1307,), std=(0.3081,), inplace=False)\n",
    "    ]\n",
    "    transform = transforms.Compose(tensor)\n",
    "    dataset1 = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=True)\n",
    "    dataset2 = datasets.MNIST(root=\"../data\", train=False, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = MNISTConvNet(conv1_in_channels=args.conv1_in_channels, conv1_out_channels=args.conv1_out_channels, conv1_kernel_size=args.conv1_kernel_size, conv1_stride=args.conv1_stride,\n",
    "        conv2_in_channels=args.conv2_in_channels, conv2_out_channels=args.conv2_out_channels, conv2_kernel_size=args.conv2_kernel_size, conv2_stride=args.conv2_stride,\n",
    "        pool1_kernel_size=args.pool1_kernel_size, dropout1_p=args.dropout1_p, dropout2_p=args.dropout2_p,\n",
    "        fullconn1_in_features=args.fullconn1_in_features, fullconn1_out_features=args.fullconn1_out_features, fullconn2_in_features=args.fullconn2_in_features, fullconn2_out_features=args.fullconn2_out_features,\n",
    "        adadelta_lr=args.adadelta_lr, adadelta_rho=args.adadelta_rho, adadelta_eps=args.adadelta_eps, adadelta_weight_decay=args.adadelta_weight_decay,\n",
    "        dataset_root=args.dataset_root, dataset_download=args.dataset_download,\n",
    "        dataloader_mean=args.dataloader_mean, dataloader_std=args.dataloader_std, dataloader_batch_size=args.dataloader_batch_size, dataloader_num_workers=args.dataloader_num_workers\n",
    "        ).to(device)\n",
    "    optimizer = torch.optim.Adadelta(params=model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        train(args=args, model=model, device=device, train_loader=train_loader, optimizer=optimizer, epoch=epoch)\n",
    "        test(model=model, device=device, test_loader=test_loader)\n",
    "        scheduler.step()\n",
    "    \n",
    "    if args.save_model:\n",
    "        torch.save(obj=model.state_dict(), f=\"mnist_cnn.pt\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "argparser = get_argparser()\n",
    "args = argparser.parse_args(\n",
    "    [\n",
    "        \"--batch-size\", str(32),\n",
    "        \"--test-batch-size\", str(32),\n",
    "        \"--epochs\", str(1),\n",
    "        \"--lr\", str(0.5),\n",
    "        \"--gamma\", str(0.7),\n",
    "        \"--no-cuda\",\n",
    "        \"--seed\", str(1),\n",
    "        \"--log-interval\", str(10),\n",
    "        \"--save-model\",\n",
    "    ]\n",
    ")\n",
    "main(args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('2021-08-09': conda)"
  },
  "interpreter": {
   "hash": "e70296657866dcab8c1fff77bde7d1eee3f4d180a603dc3f836290cb0427c466"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}